# ML_study

볼드체로 표시한 부분 → 추후에 다시 알아볼 내용

E → Ebook 기준 페이지

2022.09.20

p.46
테스트 데이터셋에서 따로 계산한 평균과 표준 편차를 사용하여 테스트 데이터셋을 전처리하면 훈련 데이터셋과는 **다른 비율로 변환**하기 때문에 훈련 데이터셋에서 학습한 모델이 잘 작동하지 않을 것이다. 새로운 데이터 하나에서 예측을 만들 때는 이런 통계를 구할 수도 없다. **테스트 데이터셋을 본적없는 새로운 데이터와 동일하게 취급하지 않으면** 낙관적인 결과를 얻게 된다. 

지도학습의 주요 하위 분야 두 개는 분류와 회귀

비지도학습은 레이블되지 않은 데이터에서 구조를 찾는 방법

예를 들어, 첫 번째 샘플 x1에 4개의 특성(차원)이 있을 때, 최종 입력인 z = w1x1 + w2x2 + w3x3 + w4x4 이다. 이를 결정 함수(보통 단위 계단 함수를 변형한 함수인)에 input으로 넣어서 출력값이 사전에 정의한 임계값보다 크거나 같으면 1, 작으면 0 으로 분류

p.55 결정경계에서의 x1,x2는 무엇을 의미?

퍼셉트론 학습 규칙

1. 가중치를 0 또는 랜덤한 작은 값으로 초기화
2. 각 훈련 샘플 x(i)에서 다음 작업을 한다.
a. 출력 값 y^을 계산한다.
b. 가중치를 업데이트한다.

퍼셉트론은 두 클래스가 선형적으로 구분되고 학습률이 충분히 작을 때만 수렴이 보장된다.
두 클래스를 선형 결정 경계로 나눌 수 없다면 훈련 데이터셋을 반복할 최대횟수(epoch)를 지정하고 분류 허용 오차를 지정할 수 있다. 그렇지 않으면 퍼셉트론은 가중치 업데이트를 멈추지 않는다. 

-> 왜냐하면, 선형 결정 경계로 데이터셋을 분류 할 수 없기 때문이다.

p.61
가중치가 0으로 초기화되어 있다면 학습률 파라미터 **eta는 가중치 벡터의 방향이 아니라 크기에만 영향을 미친다.** 

2022.09.21

E p.41

아달린 규칙에서는 가중치를 업데이트 할 때 단위 계단 함수 대신 선형 활성화 함수를 사용한다. 아달린에서 선형 활성화 함수는 항등 함수이다.

아달린 알고리즘은 진짜 클래스 레이블과 선형 활성화 함수의 실수 출력 값을 비교하여 모델의 오차를 계산하고 가중치를 업데이트한다. 

E p.49

self.w_[0]은 절편이므로 업데이트 할 때, feature를 담고 있는 행벡터와 곱하지 않는다. 

2022.09.22

p. 94~95

odds ratio = p / (1 - p)

logit(p) = log(p / (1 - p))

logit(p(y=1|x)) = w0x0 + w1x1 … + wmxm = w.T*x

p(y=1|x)는 특성 x가 주어졌을 때 이 샘플이 클래스 1에 속할 조건부 확률

어떤 샘플이 특정 클래스에 속할 확률을 예측하는 것이 목적이므로 logit함수를 거꾸로 뒤집어서 시그모이드 함수를 만든다.

sigmoid function = 1 / (1 + e^-z) → 어떻게 logit 함수를 뒤집어서 이런 시그모이드 함수를 만들 수 있는지???

시그모이드 함수의 출력은 특정 샘플이 클래스 1에 속할 확률